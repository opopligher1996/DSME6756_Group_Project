{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135d790f-7088-4f73-b965-eda0a9f06814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 18:43:43.669434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    from keras import metrics\n",
    "    from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b2d462-5c71-44d3-bd48-3ea962bd108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k=5, augment=True):\n",
    "\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load('images.npy')\n",
    "        labels = np.load('labels.npy')\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "def augment_images(images, labels):\n",
    "\n",
    "    # Create generators to augment images\n",
    "    from keras.preprocessing import image\n",
    "    flip_generator = image.ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True\n",
    "    )\n",
    "    rotate_generator = image.ImageDataGenerator(\n",
    "        rotation_range=360,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Accumulate augmented images and labels\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    # Loop each images in the set to augment\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        # Reshape image for generator\n",
    "        image = np.reshape(images[i], (1, images[i].shape[0], images[i].shape[1], 1))\n",
    "        label = labels[i]\n",
    "\n",
    "        # Reset the number of augmented images have been created to zero\n",
    "        num_new_images = 0\n",
    "\n",
    "        # Generate 2 new images if the image is of a tropical cyclone between 50 and 75 knots\n",
    "        if 50 < label < 75:\n",
    "            for batch in flip_generator.flow(image, batch_size=1):\n",
    "                gc.collect()\n",
    "                new_image = np.reshape(batch[0], (batch[0].shape[0], batch[0].shape[1], 1))\n",
    "                augmented_images.append(new_image)\n",
    "                augmented_labels.append(label)\n",
    "                num_new_images += 1\n",
    "                if num_new_images == 2:\n",
    "                    break\n",
    "\n",
    "        # Generate 6 new images if the image is of a tropical cyclone between 75 and 100 knots\n",
    "        elif 75 < label < 100:\n",
    "            for batch in rotate_generator.flow(image, batch_size=1):\n",
    "                gc.collect()\n",
    "                new_image = np.reshape(batch[0], (batch[0].shape[0], batch[0].shape[1], 1))\n",
    "                augmented_images.append(new_image)\n",
    "                augmented_labels.append(label)\n",
    "                num_new_images += 1\n",
    "                if num_new_images == 6:\n",
    "                    break\n",
    "\n",
    "        # Generate 12 new images if the image is of a tropical cyclone greater than or equal to 100 knots\n",
    "        elif 100 <= label:\n",
    "            for batch in rotate_generator.flow(image, batch_size=1):\n",
    "                gc.collect()\n",
    "                new_image = np.reshape(batch[0], (batch[0].shape[0], batch[0].shape[1], 1))\n",
    "                augmented_images.append(new_image)\n",
    "                augmented_labels.append(label)\n",
    "                num_new_images += 1\n",
    "                if num_new_images == 12:\n",
    "                    break\n",
    "\n",
    "        print_progress('Augmenting Images', i + 1, len(images))\n",
    "\n",
    "    # Convert lists of images/labels into numpy arrays\n",
    "    augmented_images = np.array(augmented_images)\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "    return augmented_images, augmented_labels\n",
    "\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    # Build network architecture\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation=None))\n",
    "\n",
    "    # Configure model optimization\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=[metrics.MeanAbsoluteError(), metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_images, train_labels, test_images, test_labels, show_performance_by_epoch=False):\n",
    "\n",
    "    # Run model and get metrics for each epoch\n",
    "    performance_log = model.fit(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        callbacks=[EarlyStopping(monitor='val_mean_absolute_error', patience=15, restore_best_weights=True)],\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_data=(test_images, test_labels),\n",
    "        shuffle = True)\n",
    "\n",
    "    if show_performance_by_epoch:\n",
    "        performance_by_epoch(performance_log)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def performance_by_epoch(performance_log):\n",
    "\n",
    "    # Get metrics for each epoch after model finishes training\n",
    "    train_loss = performance_log.history['loss']\n",
    "    test_loss = performance_log.history['val_loss']\n",
    "    train_mae = performance_log.history['mean_absolute_error']\n",
    "    test_mae = performance_log.history['val_mean_absolute_error']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Build a dataframe storing epoch metrics\n",
    "    performance_df = pd.DataFrame(columns=['epoch', 'train_or_test', 'loss_or_mae', 'value'])\n",
    "    for i in range(len(train_loss)):\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'train', 'loss_or_mae': 'loss', 'value': train_loss[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'test', 'loss_or_mae': 'loss', 'value': test_loss[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'train', 'loss_or_mae': 'mae', 'value': train_mae[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'test', 'loss_or_mae': 'mae', 'value': test_mae[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "    performance_df = performance_df.astype({'epoch': np.int64})\n",
    "\n",
    "    # Plot metrics on graph, fitted with exponential decay curves\n",
    "    lm = sns.lmplot(\n",
    "        x='epoch',\n",
    "        y='value',\n",
    "        data=performance_df,\n",
    "        row='loss_or_mae',\n",
    "        hue='train_or_test',  # Note: If epoch = 1, this line causes an error. Make sure epoch >= 2\n",
    "        logx=True,\n",
    "        truncate=False,\n",
    "        sharey=False)\n",
    "    axes = lm.axes\n",
    "    max_mae = performance_df.loc[performance_df.loss_or_mae == 'mae']['value'].max()\n",
    "    min_mae = performance_df.loc[performance_df.loss_or_mae == 'mae']['value'].min()\n",
    "    axes[1, 0].set_ylim(min_mae - min_mae * 0.2, max_mae + max_mae * 0.2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_predictions(model, test_images, test_labels):\n",
    "\n",
    "    # Run validation data through model and print mean absolute error\n",
    "    raw_predictions = model.predict(test_images)\n",
    "    raw_predictions = raw_predictions.flatten()\n",
    "\n",
    "    # Build a dataframe storing data for each prediction made by the model\n",
    "    processed_predictions = pd.DataFrame(columns=['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(len(raw_predictions)):\n",
    "        abs_error = abs(raw_predictions[i] - test_labels[i])\n",
    "        new_row = {\n",
    "            'prediction': raw_predictions[i],\n",
    "            'actual': test_labels[i],\n",
    "            'abs_error': abs_error,\n",
    "            'abs_error_squared': abs_error ** 2,\n",
    "            'category': category_of(test_labels[i])}\n",
    "        processed_predictions = processed_predictions.append(new_row, ignore_index=True)\n",
    "        print_progress('Processing Predictions', i + 1, len(raw_predictions))\n",
    "\n",
    "    return processed_predictions\n",
    "\n",
    "\n",
    "def show_validation_results(predictions, show_plots=True, print_error=True):\n",
    "\n",
    "    print('\\n\\nRESULTS')\n",
    "\n",
    "    if print_error:\n",
    "        mae = predictions['abs_error'].mean()\n",
    "        print('\\nMean Absolute Error: ' + str(round(float(mae), 2)) + ' knots')\n",
    "        rmse = predictions['abs_error_squared'].mean() ** 0.5\n",
    "        print('Root Mean Square Error: ' + str(round(float(rmse), 2)) + ' knots')\n",
    "\n",
    "    if show_plots:\n",
    "        # List of categories in order of ascending strength\n",
    "        categories = ['T. Depression', 'T. Storm', 'Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5']\n",
    "\n",
    "        # Show bar graph of median absolute error for each category\n",
    "        plt.figure(figsize=(10, 5), dpi=300)\n",
    "        sns.barplot(\n",
    "            x='category',\n",
    "            y='abs_error',\n",
    "            data=predictions,\n",
    "            estimator=np.median,\n",
    "            order=categories)\n",
    "        sns.despine()\n",
    "        plt.xlabel(\"Hurricane Strength\")\n",
    "        plt.ylabel(\"Absolute Error\")\n",
    "        plt.title(\"Median Absolute Error in Neural Network's Predictions By Category\")\n",
    "        plt.savefig('median_abs_error_by_category.png')\n",
    "        print('Graph of median absolute error by category saved as median_abs_error_by_category.png')\n",
    "        plt.clf()\n",
    "\n",
    "        # Show density plot of error for each category\n",
    "        for category in categories:\n",
    "            num_samples_tested = len(predictions.loc[predictions.category == category]['abs_error'])\n",
    "            sns.distplot(\n",
    "                predictions.loc[predictions.category == category]['abs_error'],\n",
    "                label=category + ' (' + str(num_samples_tested) + ' samples tested)',\n",
    "                hist=False,\n",
    "                kde_kws={\"shade\": True})\n",
    "            sns.despine()\n",
    "        plt.xlabel(\"Absolute Error\")\n",
    "        plt.title(\"Distribution of Absolute Error By Category\")\n",
    "        plt.legend()\n",
    "        plt.xlim(0, None)\n",
    "        plt.ylim(0, None)\n",
    "        plt.savefig('error_dist_by_category.png')\n",
    "        print('Graph of error distribution by category saved as error_dist_by_category.png')\n",
    "\n",
    "\n",
    "def standardize_data(train_images, test_images):\n",
    "    train_images[train_images < 0] = 0\n",
    "    test_images[test_images < 0] = 0\n",
    "    st_dev = np.std(train_images)\n",
    "    mean = np.mean(train_images)\n",
    "    train_images = np.divide(np.subtract(train_images, mean), st_dev)\n",
    "    test_images = np.divide(np.subtract(test_images, mean), st_dev)\n",
    "    return train_images, test_images\n",
    "\n",
    "\n",
    "def print_progress(action, progress, total):\n",
    "    percent_progress = round((progress / total) * 100, 1)\n",
    "    print('\\r' + action + '... ' + str(percent_progress) + '% (' + str(progress) + ' of ' + str(total) + ')', end='')\n",
    "\n",
    "\n",
    "def category_of(wind_speed):\n",
    "    if wind_speed <= 33:\n",
    "        return 'T. Depression'\n",
    "    elif wind_speed <= 64:\n",
    "        return 'T. Storm'\n",
    "    elif wind_speed <= 83:\n",
    "        return 'Category 1'\n",
    "    elif wind_speed <= 95:\n",
    "        return 'Category 2'\n",
    "    elif wind_speed <= 113:\n",
    "        return 'Category 3'\n",
    "    elif wind_speed <= 134:\n",
    "        return 'Category 4'\n",
    "    else:\n",
    "        return 'Category 5'\n",
    "\n",
    "\n",
    "def evaluate_and_save(model,num_of_fold,x_test,y_test):\n",
    "    # Evaluate\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0],'Test MAE:', score[1],'Test RMSE:', score[2])\n",
    "    Loss.append(score[0])\n",
    "    MAE.append(score[1])\n",
    "    RMSE.append(score[2])\n",
    "    print(\"save score...\")\n",
    "\n",
    "    # Save model\n",
    "    output_name = str(num_of_fold + 1) + \"_fold_model.h5\"\n",
    "    model.save(output_name)\n",
    "    print(\"Saved:\",output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb391d48-f673-4889-936e-3a4fe9954525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmenting Fold 1 of 5\n",
      "Augmenting Images... 100.0% (198 of 198)\n",
      "Augmenting Fold 2 of 5\n",
      "Augmenting Images... 100.0% (198 of 198)\n",
      "Augmenting Fold 3 of 5\n",
      "Augmenting Images... 100.0% (198 of 198)\n",
      "Augmenting Fold 4 of 5\n",
      "Augmenting Images... 100.0% (198 of 198)\n",
      "Augmenting Fold 5 of 5\n",
      "Augmenting Images... 100.0% (198 of 198)\n",
      "\n",
      "Training Fold 1 of 5\n",
      "\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 10s 288ms/step - loss: 1348.7756 - mean_absolute_error: 27.9398 - root_mean_squared_error: 36.7257 - val_loss: 1666.5336 - val_mean_absolute_error: 36.2983 - val_root_mean_squared_error: 40.8232\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 9s 314ms/step - loss: 250.5871 - mean_absolute_error: 12.5077 - root_mean_squared_error: 15.8299 - val_loss: 932.0407 - val_mean_absolute_error: 25.5261 - val_root_mean_squared_error: 30.5293\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 7s 266ms/step - loss: 171.6204 - mean_absolute_error: 10.4541 - root_mean_squared_error: 13.1004 - val_loss: 526.1512 - val_mean_absolute_error: 17.4564 - val_root_mean_squared_error: 22.9380\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 7s 268ms/step - loss: 160.0844 - mean_absolute_error: 9.9575 - root_mean_squared_error: 12.6524 - val_loss: 341.3060 - val_mean_absolute_error: 14.0752 - val_root_mean_squared_error: 18.4745\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 147.9449 - mean_absolute_error: 9.7178 - root_mean_squared_error: 12.1633 - val_loss: 293.5643 - val_mean_absolute_error: 12.8867 - val_root_mean_squared_error: 17.1337\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 9s 334ms/step - loss: 131.8414 - mean_absolute_error: 9.1249 - root_mean_squared_error: 11.4822 - val_loss: 269.4546 - val_mean_absolute_error: 12.8919 - val_root_mean_squared_error: 16.4151\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 6s 229ms/step - loss: 128.1866 - mean_absolute_error: 9.0333 - root_mean_squared_error: 11.3219 - val_loss: 294.4878 - val_mean_absolute_error: 12.4985 - val_root_mean_squared_error: 17.1606\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 12s 435ms/step - loss: 105.0295 - mean_absolute_error: 8.1089 - root_mean_squared_error: 10.2484 - val_loss: 258.6667 - val_mean_absolute_error: 12.1529 - val_root_mean_squared_error: 16.0831\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 14s 494ms/step - loss: 114.2096 - mean_absolute_error: 8.4815 - root_mean_squared_error: 10.6869 - val_loss: 282.1605 - val_mean_absolute_error: 12.5081 - val_root_mean_squared_error: 16.7976\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 14s 493ms/step - loss: 101.9069 - mean_absolute_error: 8.0407 - root_mean_squared_error: 10.0949 - val_loss: 323.1103 - val_mean_absolute_error: 13.2235 - val_root_mean_squared_error: 17.9753\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 9s 321ms/step - loss: 99.5343 - mean_absolute_error: 7.9072 - root_mean_squared_error: 9.9767 - val_loss: 223.2455 - val_mean_absolute_error: 11.3476 - val_root_mean_squared_error: 14.9414\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 7s 244ms/step - loss: 88.1518 - mean_absolute_error: 7.3847 - root_mean_squared_error: 9.3889 - val_loss: 165.2239 - val_mean_absolute_error: 10.0580 - val_root_mean_squared_error: 12.8539\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 18s 637ms/step - loss: 77.6270 - mean_absolute_error: 6.9633 - root_mean_squared_error: 8.8106 - val_loss: 240.2386 - val_mean_absolute_error: 13.1165 - val_root_mean_squared_error: 15.4996\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 14s 516ms/step - loss: 74.3298 - mean_absolute_error: 6.6873 - root_mean_squared_error: 8.6215 - val_loss: 205.3228 - val_mean_absolute_error: 11.9392 - val_root_mean_squared_error: 14.3291\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 8s 279ms/step - loss: 71.8716 - mean_absolute_error: 6.6204 - root_mean_squared_error: 8.4777 - val_loss: 135.4634 - val_mean_absolute_error: 8.8393 - val_root_mean_squared_error: 11.6389\n",
      "Epoch 16/100\n",
      "19/28 [===================>..........] - ETA: 2s - loss: 80.3468 - mean_absolute_error: 7.0393 - root_mean_squared_error: 8.9636"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify whether the script should use Keras's ImageDataGenerator to augment the training dataset. Assigning\n",
    "    # this variable to True will improve accuracy, but will also increase execution time.\n",
    "    AUGMENT = True\n",
    "\n",
    "    # Specify how many folds in the k-fold validation process. Can be any integer greater than or equal to 2. Larger\n",
    "    # integers will increase execution time.\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment=AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns=['prediction', 'actual', 'abs_error', 'category'])\n",
    "\n",
    "    \n",
    "    Loss = []\n",
    "    MAE = []\n",
    "    RMSE = []\n",
    "    \n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "\n",
    "        evaluate_and_save(model,i,test_images[i], test_labels[i])\n",
    "        \n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        # save csv\n",
    "        kth_fold_predictions.to_csv(str(i+1) + \"_pred.csv\")\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index=True)\n",
    "\n",
    "    predictions.to_csv(\"all_pred.csv\")\n",
    "    show_validation_results(predictions)\n",
    "    \n",
    "    k_fold_result = {\"k_fold\":[1,2,3,4,5],\"Loss\":Loss,\"MAE\":MAE,\"RMSE\":RMSE}\n",
    "    k_fold_result_df = pd.DataFrame(k_fold_result)\n",
    "    k_fold_result_df.to_csv(\"k_fold_result.csv\")\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print(\"#\"*7,\"Fold {} Result\".format(i+1),\"#\"*7)\n",
    "        print('Test loss:', Loss[i],'Test MAE:', MAE[i],'Test RMSE:', RMSE[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601081f-3ebd-4dec-b6b2-4addd15b0c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
